{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signed Graph Convolutional Network(SGCN)を用いたFraud User Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sgcn_unsupervised import SignedGCNTrainer, SignedGCNPredictor\n",
    "from parser import parameter_parser\n",
    "from utils import tab_printer, read_graph, score_printer, save_logs\n",
    "import easydict\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":300,\n",
    "        \"test_size\":0.2,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.01,  \n",
    "        \"weight_decay\": 0.0, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32, 32, ],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tab_printer(args)\n",
    "edges, nodes_dict = read_graph(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = SignedGCNTrainer(args, edges, nodes_dict)\n",
    "trainer.setup_dataset()\n",
    "trainer.create_and_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.test_size > 0:\n",
    "    display(pd.DataFrame(trainer.logs['performance']))\n",
    "    # save_logs(args, trainer.logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = trainer.model.z.cpu().detach().numpy()\n",
    "# Z = trainer.model.z_1.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "clusters = kmeans.fit_predict(Z)\n",
    "clusters_score = kmeans.transform(Z)\n",
    "clusters_prob = torch.softmax(torch.from_numpy(1/clusters_score),dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.unique(clusters,return_counts=True))\n",
    "print(clusters_score[0],clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(clusters==clusters_score.argmin(1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(clusters==clusters_prob.argmax(1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_scores = clusters_score[:,1]\n",
    "benign_scores = clusters_score[:,0]\n",
    "fraud_prob = clusters_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt_df = pd.DataFrame(nodes_dict).drop('all_ncount',1)\n",
    "\n",
    "gt_df['label'] = gt_df.label.map({1:0,-1:1})\n",
    "\n",
    "gt_pred_df = pd.merge(gt_df,pd.DataFrame(fraud_scores,columns=['fraud_score']),left_on='indice', right_index=True,how='left')\n",
    "gt_pred_df = pd.merge(gt_pred_df,pd.DataFrame(benign_scores,columns=['benign_score']),left_on='indice', right_index=True,how='left')\n",
    "gt_pred_df = pd.merge(gt_pred_df,pd.DataFrame(fraud_prob,columns=['fraud_prob']),left_on='indice', right_index=True,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(average_precision_score(y_true=gt_pred_df.sort_values('fraud_prob',ascending=False).iloc[:100].label,\n",
    "                                                y_score=gt_pred_df.sort_values('fraud_prob',ascending=False).iloc[:100].fraud_prob))\n",
    "\n",
    "print(average_precision_score(y_true=1-gt_pred_df.sort_values('fraud_prob').iloc[:100].label,\n",
    "                                                y_score=1-gt_pred_df.sort_values('fraud_prob').iloc[:100].fraud_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(average_precision_score(y_true=gt_pred_df.sort_values('fraud_score').iloc[:100].label,\n",
    "                                                y_score=gt_pred_df.sort_values('fraud_score').iloc[:100].fraud_score))\n",
    "\n",
    "print(average_precision_score(y_true=gt_pred_df.sort_values('benign_score').iloc[:100].label,\n",
    "                                                y_score=gt_pred_df.sort_values('benign_score').iloc[:100].benign_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inductive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":300,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32, 16, ],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.0,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edges, new_nodes_dict = read_graph(new_args)\n",
    "\n",
    "X = np.array(pd.read_csv(f'../input/{data_name}/{data_name}_node_feature.csv')) # general node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dataset = input('学習に使ったデータセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = SignedGCNPredictor(new_args, f'../output/inductive/{training_dataset}_model', X, new_edges,new_nodes_dict)\n",
    "\n",
    "predictions = predictor.predict()\n",
    "predict_labels = predictions.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_node_raw = np.load(f'../input/{training_dataset}/{training_dataset}_label_encoder.npy')\n",
    "\n",
    "newly_added_node_judger = ~np.isin(new_nodes_dict['indice'],trained_node_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = new_nodes_dict['label'][newly_added_node_judger]\n",
    "y_score_indice = new_nodes_dict['indice'][newly_added_node_judger]\n",
    "\n",
    "roc_auc_score(y_true=[1 if i==-1 else 0 for i in y_true],y_score=predictions[y_score_indice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "confusion_matrix([1 if i==-1 else 0 for i in new_nodes_dict['label']],predict_labels[new_nodes_dict['indice']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(predictions[new_nodes_dict['indice']][y_true==1],alpha=0.5,bins=10)\n",
    "_ = plt.hist(predictions[new_nodes_dict['indice']][y_true==-1],alpha=0.5,bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロバストネスの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = input('データセット：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "        \"edge_path\": f'../input/{data_name}/{data_name}_network.csv',\n",
    "        \"features_path\":  f'../input/{data_name}/{data_name}_node_feature.csv',\n",
    "        \"nodes_path\": f'../input/{data_name}/{data_name}_gt.csv',\n",
    "        \"embedding_path\": f'../tmp/embedding/{data_name}_sgcn_feature05.pkl', # tmp folder for cross-validation\n",
    "        \"regression_weights_path\": f'../tmp/weights/{data_name}_sgcn_feature05.pkl',\n",
    "        \"inductive_model_path\": None, # f'../output/inductive/{data_name}_model', # or None\n",
    "        \"log_path\": f'../logs/{data_name}_logs_feature05.json',\n",
    "        \"epochs\":100,\n",
    "        \"test_size\":0.33,\n",
    "        \"reduction_iterations\": 128,\n",
    "        \"reduction_dimensions\": 30,\n",
    "        \"seed\": 42,\n",
    "        \"lamb\": 0.0,\n",
    "        \"learning_rate\": 0.001,  \n",
    "        \"weight_decay\": 10e-4, \n",
    "        # \"layers\": [64, 32,16,8],\n",
    "        \"layers\": [32,16,],\n",
    "        \"spectral_features\":False,\n",
    "        \"general_features\": True,  \n",
    "        \"sample_num\":None,\n",
    "        \"class_weights\":False,\n",
    "        \"node_under_sampling\":False,\n",
    "        \"hidden_residual\":False,\n",
    "        \"eval_freq\":1,\n",
    "        \"subgraph_training\":False,\n",
    "        \"l1_lambda\":0.05,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_printer(args)\n",
    "edges, nodes_dict = read_graph(args) # nodes_dict['indice']:node_id , nodes_dict['label'] : label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indice = nodes_dict['indice']\n",
    "all_labels = nodes_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robustness_experiments(training_rates_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    all_auc_scores = []\n",
    "    done_train_rate = []\n",
    "    training_rates = training_rates_list\n",
    "    for train_rate in training_rates:\n",
    "        auc_scores = []\n",
    "        for i in range(30): \n",
    "            train_index, test_index = train_test_split(np.arange(len(nodes_dict['indice'])),\n",
    "                                                       stratify=nodes_dict['label'],train_size=float(train_rate),shuffle=True)\n",
    "            print(\"==== Training Phase ====\")\n",
    "            print(f'{i}-th')\n",
    "            # training\n",
    "            train_node_indice = all_indice[train_index]\n",
    "            train_node_labels = all_labels[train_index]\n",
    "            print(f'labels:{np.unique(train_node_labels,return_counts=True)}')\n",
    "            tmp_nodes_dict = {}\n",
    "            tmp_nodes_dict['all_ncount'] = nodes_dict['all_ncount']\n",
    "            tmp_nodes_dict['indice'] = train_node_indice\n",
    "            tmp_nodes_dict['label'] = train_node_labels\n",
    "            trainer = SignedGCNTrainer(args, edges, tmp_nodes_dict)\n",
    "            trainer.setup_dataset()\n",
    "            trainer.create_and_train_model()\n",
    "\n",
    "            if args.test_size > 0:\n",
    "                # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "                # score_printer(trainer.logs)\n",
    "                display(pd.DataFrame(trainer.logs['performance']))\n",
    "                save_logs(args, trainer.logs)\n",
    "\n",
    "            # test\n",
    "            print(\"==== Test Phase ====\")\n",
    "            test_node_indice = all_indice[test_index]\n",
    "            test_node_labels = all_labels[test_index]\n",
    "            # feature = pd.read_csv(args.embedding_path,index_col='id').values\n",
    "            feature = pd.read_pickle(args.embedding_path).drop('id',1).values\n",
    "            test_feature = feature[test_node_indice]\n",
    "            # weight = pd.read_csv(args.regression_weights_path)\n",
    "            weight = pd.read_pickle(args.regression_weights_path)\n",
    "            predictions = np.dot(test_feature,weight.values.T)\n",
    "            probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions)).numpy()\n",
    "            predict_labels = probabilities.argmax(1)\n",
    "            auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=probabilities[:,1])\n",
    "            auc_scores.append(auc_score)\n",
    "            cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels)\n",
    "            print(f\"{i}-th fold's auc_score:{auc_score}\")\n",
    "            print(cmx)\n",
    "            print()\n",
    "        print(f\"train_rate : {train_rate} --> {np.mean(auc_scores)}\")\n",
    "        done_train_rate.append(train_rate)\n",
    "        all_auc_scores.append(np.mean(auc_scores))\n",
    "        pd.DataFrame(all_auc_scores,index=done_train_rate).to_csv('../logs/{}_tmp'.format(data_name))\n",
    "    return pd.DataFrame(all_auc_scores, index=training_rates, columns=['average_auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#result_df = robustness_experiments([0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.03])\n",
    "result_df = robustness_experiments([0.03,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.to_csv('amazon_robustness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rate = input('トレーニングデータの比率：')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "for i in range(10): # trainとtestを逆にする\n",
    "    train_index, test_index = train_test_split(np.arange(len(nodes_dict['indice'])),stratify=nodes_dict['label'],train_size=float(train_rate)\n",
    "                                               ,shuffle=True)\n",
    "    print(\"==== Training Phase ====\")\n",
    "    print(f'{i}-th')\n",
    "    # training\n",
    "    train_node_indice = all_indice[train_index]\n",
    "    train_node_labels = all_labels[train_index]\n",
    "    print(f'labels:{np.unique(train_node_labels,return_counts=True)}')\n",
    "    tmp_nodes_dict = {}\n",
    "    tmp_nodes_dict['all_ncount'] = nodes_dict['all_ncount']\n",
    "    tmp_nodes_dict['indice'] = train_node_indice\n",
    "    tmp_nodes_dict['label'] = train_node_labels\n",
    "    trainer = SignedGCNTrainer(args, edges, tmp_nodes_dict)\n",
    "    trainer.setup_dataset()\n",
    "    trainer.create_and_train_model()\n",
    "    \n",
    "    if args.test_size > 0:\n",
    "        # trainer.save_model() ## trainer.create_and_train_model()のなかで，すでにbest_modelが保存されている．\n",
    "        # score_printer(trainer.logs)\n",
    "        display(pd.DataFrame(trainer.logs['performance']))\n",
    "        save_logs(args, trainer.logs)\n",
    "\n",
    "    # test\n",
    "    print(\"==== Test Phase ====\")\n",
    "    test_node_indice = all_indice[test_index]\n",
    "    test_node_labels = all_labels[test_index]\n",
    "    # feature = pd.read_csv(args.embedding_path,index_col='id').values\n",
    "    feature = pd.read_pickle(args.embedding_path).drop('id',1).values\n",
    "    test_feature = feature[test_node_indice]\n",
    "    # weight = pd.read_csv(args.regression_weights_path)\n",
    "    weight = pd.read_pickle(args.regression_weights_path)\n",
    "    predictions = np.dot(test_feature,weight.values.T)\n",
    "    probabilities = torch.nn.functional.softmax(torch.from_numpy(predictions)).numpy()\n",
    "    predict_labels = probabilities.argmax(1)\n",
    "    auc_score = roc_auc_score(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_score=probabilities[:,1])\n",
    "    auc_scores.append(auc_score)\n",
    "    cmx = confusion_matrix(y_true=[1 if i==-1 else 0 for i in test_node_labels],y_pred=predict_labels)\n",
    "    print(f\"{i}-th fold's auc_score:{auc_score}\")\n",
    "    print(cmx)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
