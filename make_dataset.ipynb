{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "early_amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGCN用データセット作り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(df,col,rating_cols=None):\n",
    "    df_cnt = df.groupby([col]+['rating'])['time'].count().unstack(1,fill_value=0)\n",
    "    df_dist = pd.DataFrame(df_cnt.values / df_cnt.sum(1).values.reshape(-1,1),\n",
    "                                               columns=df_cnt.columns,\n",
    "                                               index=df_cnt.index)\n",
    "    if rating_cols is not None:\n",
    "        df_dist = pd.concat([pd.DataFrame(columns=rating_cols),df_dist]).fillna(0)\n",
    "    return df_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon → SGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-product network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_dir = 'amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "amazon_network = pd.read_csv('raw_data/{0}/{0}_network.csv'.format(amazon_dir),header=None)\n",
    "amazon_network.columns = ['user_id','product_id','rating','time']\n",
    "amazon_network['weight'] = amazon_network.rating.map(lambda x:(x-3)/2).round()\n",
    "\n",
    "amazon_gt = pd.read_csv('raw_data/{0}/{0}_gt.csv'.format(amazon_dir),header=None)\n",
    "amazon_gt.columns = ['user_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_amazon_network = amazon_network.loc[amazon_network.weight!=0,['user_id','product_id','weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(np.hstack((truncated_amazon_network.user_id,\n",
    "                                                   truncated_amazon_network.product_id,\n",
    "                                                   amazon_gt.user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_amazon_network['id1'] = label_encoder.transform(truncated_amazon_network.user_id)\n",
    "\n",
    "truncated_amazon_network['id2'] = label_encoder.transform(truncated_amazon_network.product_id)\n",
    "\n",
    "amazon_gt['node_id'] = label_encoder.transform(amazon_gt.user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dist = get_dist(amazon_network,'user_id')\n",
    "\n",
    "product_dist = get_dist(amazon_network,'product_id')\n",
    "\n",
    "# user_product_dist = user_dist.append(product_dist)\n",
    "\n",
    "user_product_dist = pd.concat([user_dist,product_dist],1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "node_features_df = user_product_dist.loc[label_encoder.classes_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ファイル出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_amazon_network[['id1','id2','weight']].to_csv('input/{0}/{0}_network.csv'.format(amazon_dir),index=None)\n",
    "\n",
    "amazon_gt[['node_id','label']].to_csv('input/{0}/{0}_gt.csv'.format(amazon_dir),index=None)\n",
    "\n",
    "np.save(arr=label_encoder.classes_,file='input/{0}/{0}_label_encoder.npy'.format(amazon_dir))\n",
    "\n",
    "node_features_df.to_csv('input/{0}/{0}_node_feature.csv'.format(amazon_dir),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epinions_network = pd.read_csv('raw_data/epinions/epinions_network.csv',header=None)\n",
    "\n",
    "epinions_network.columns = ['id1','id2','rating','time']\n",
    "\n",
    "epinions_network['weight'] = epinions_network.rating.map(lambda x:-1 if x-3.5 < 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epinions_gt = pd.read_csv('raw_data/epinions/epinions_gt.csv',header=None)\n",
    "\n",
    "epinions_gt.columns = ['user_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epinions_gt = epinions_gt.loc[epinions_gt.user_id.isin(set(epinions_network.id1.unique()) | set(epinions_network.id2.unique()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(np.hstack((epinions_network.id1,\n",
    "                                                   epinions_network.id2,\n",
    "                                                   epinions_gt.user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epinions_network['id1_'] = label_encoder.transform(epinions_network.id1)\n",
    "\n",
    "epinions_network['id2_'] = label_encoder.transform(epinions_network.id2)\n",
    "\n",
    "epinions_gt['node_id'] = label_encoder.transform(epinions_gt.user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_df = pd.concat([get_dist(epinions_network,'id1_'),get_dist(epinions_network,'id2_')],1).fillna(0).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイル出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epinions_network[['id1_','id2_','weight']].to_csv('input/epinions/epinions_network.csv',index=None)\n",
    "epinions_gt[['node_id','label']].to_csv('input/epinions/epinions_gt.csv',index=None)\n",
    "np.save(arr=label_encoder.classes_,file='input/epinions/epinions_label_encoder.npy')\n",
    "node_features_df.to_csv('input/epinions/epinions_node_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## epinions_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_nodes = np.random.choice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_network = pd.read_csv('raw_data/alpha/alpha_network.csv',header=None)\n",
    "\n",
    "alpha_network.columns = ['id1','id2','rating','time']\n",
    "\n",
    "alpha_network['weight'] = alpha_network.rating.map(lambda x:1 if x>0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(alpha_network.time,unit='s').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_gt = pd.read_csv('raw_data/alpha/alpha_gt.csv',header=None)\n",
    "\n",
    "alpha_gt.columns = ['user_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(np.hstack((alpha_network.id1,\n",
    "                                                   alpha_network.id2,\n",
    "                                                   alpha_gt.user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_network['id1_'] = label_encoder.transform(alpha_network.id1)\n",
    "\n",
    "alpha_network['id2_'] = label_encoder.transform(alpha_network.id2)\n",
    "\n",
    "alpha_gt['node_id'] = label_encoder.transform(alpha_gt.user_id)\n",
    "\n",
    "node_features_df = pd.concat([get_dist(alpha_network,'id1_'),get_dist(alpha_network,'id2_')],1).fillna(0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_network[['id1_','id2_','weight']].to_csv('input/alpha/alpha_network.csv',index=None)\n",
    "alpha_gt[['node_id','label']].to_csv('input/alpha/alpha_gt.csv',index=None)\n",
    "np.save(arr=label_encoder.classes_,file='input/alpha/alpha_label_encoder.npy')\n",
    "node_features_df.to_csv('input/alpha/alpha_node_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## otc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otc_network = pd.read_csv('raw_data/otc/otc_network.csv',header=None)\n",
    "\n",
    "otc_network.columns = ['id1','id2','rating','time']\n",
    "\n",
    "otc_network['weight'] = otc_network.rating.map(lambda x:1 if x>0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otc_gt = pd.read_csv('raw_data/otc/otc_gt.csv',header=None)\n",
    "\n",
    "otc_gt.columns = ['user_id','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(np.hstack((otc_network.id1,\n",
    "                                                   otc_network.id2,\n",
    "                                                   otc_gt.user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otc_network['id1_'] = label_encoder.transform(otc_network.id1)\n",
    "\n",
    "otc_network['id2_'] = label_encoder.transform(otc_network.id2)\n",
    "\n",
    "otc_gt['node_id'] = label_encoder.transform(otc_gt.user_id)\n",
    "\n",
    "node_features_df = pd.concat([get_dist(otc_network,'id1_'),get_dist(otc_network,'id2_')],1).fillna(0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otc_network[['id1_','id2_','weight']].to_csv('input/otc/otc_network.csv',index=None)\n",
    "otc_gt[['node_id','label']].to_csv('input/otc/otc_gt.csv',index=None)\n",
    "np.save(arr=label_encoder.classes_,file='input/otc/otc_label_encoder.npy')\n",
    "node_features_df.to_csv('input/otc/otc_node_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最初のr割だけを取ってくる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_early_amazon(data_name,rate=0.1):\n",
    "    df_ = pd.read_csv(f'raw_data/{data_name}/{data_name}_network.csv',header=None)\n",
    "    df_.columns = ['id1','id2','rating','time']\n",
    "    df_['weight'] = df_.rating.map(lambda x:(x-3)/2).round()\n",
    "    \n",
    "    \n",
    "    # ここで最初の1割だけを切り取る\n",
    "    df = df_.sort_values('time').iloc[:int(len(df_)*rate),:]\n",
    "    trunc_df = df.loc[df.weight!=0]\n",
    "    \n",
    "    gt_df = pd.read_csv(f'raw_data/{data_name}/{data_name}_gt.csv',header=None)\n",
    "    gt_df.columns = ['user_id','label']    \n",
    "    \n",
    "    # gtの方も切り取る\n",
    "    gt_df = gt_df.loc[gt_df.user_id.isin(set(trunc_df.id1) | set(trunc_df.id2))]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(np.hstack((trunc_df.id1,trunc_df.id2,gt_df.user_id)))\n",
    "    \n",
    "    trunc_df['id1_'] = label_encoder.transform(trunc_df.id1)\n",
    "    trunc_df['id2_'] = label_encoder.transform(trunc_df.id2)\n",
    "    gt_df['node_id'] = label_encoder.transform(gt_df.user_id)\n",
    "    \n",
    "    \n",
    "    early_node_features_df = pd.concat([get_dist(df_,'id1'),\n",
    "                                                                       get_dist(df_,'id2')],1).fillna(0).sort_index()\n",
    "    early_node_features_df = early_node_features_df.loc[label_encoder.classes_]\n",
    "    \n",
    "    # assert f'{data_name}_early' not in os.listdir('input'), \"Directory already exists\"\n",
    "    # assert gt_df['label'].value_counts().min()>=10, \"Cannot do 10-fold cross validation\"\n",
    "    print(gt_df.label.value_counts())\n",
    "    os.mkdir(f'input/{data_name}_early{rate}')\n",
    "    trunc_df[['id1_','id2_','weight']].to_csv(f'input/{data_name}_early{rate}/{data_name}_early{rate}_network.csv',index=None)\n",
    "    gt_df[['node_id','label']].to_csv(f'input/{data_name}_early{rate}/{data_name}_early{rate}_gt.csv',index=None)\n",
    "    np.save(arr=label_encoder.classes_,file=f'input/{data_name}_early{rate}/{data_name}_early{rate}_label_encoder.npy')\n",
    "    early_node_features_df.to_csv(f'input/{data_name}_early{rate}/{data_name}_early{rate}_node_feature.csv',index=None)    \n",
    "    \n",
    "    \n",
    "    return trunc_df, gt_df, early_node_features_df, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_early_network(data_name,rate=0.1):\n",
    "    df_ = pd.read_csv(f'raw_data/{data_name}/{data_name}_network.csv',header=None)\n",
    "    df_.columns = ['id1','id2','rating','time']\n",
    "    if (data_name=='otc') or (data_name=='alpha'):\n",
    "        df_['weight'] = df_.rating.map(lambda x:1 if x>0 else -1)\n",
    "    elif data_name == 'epinions':\n",
    "        df_['weight'] = df_.rating.map(lambda x:-1 if x-3.5 < 0 else 1)\n",
    "    elif data_name == 'amazon':\n",
    "        return make_early_amazon(data_name,rate=rate)\n",
    "    else:\n",
    "        assert False, 'Not Supported'\n",
    "    # ここで最初の1割だけを切り取る\n",
    "    df = df_.sort_values('time').iloc[:int(len(df_)*rate),:]\n",
    "    \n",
    "    gt_df = pd.read_csv(f'raw_data/{data_name}/{data_name}_gt.csv',header=None)\n",
    "    gt_df.columns = ['user_id','label']    \n",
    "    \n",
    "    # gtの方も切り取る\n",
    "    gt_df = gt_df.loc[gt_df.user_id.isin(set(df.id1) | set(df.id2))]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(np.hstack((df.id1,df.id2,gt_df.user_id)))\n",
    "    \n",
    "    df['id1_'] = label_encoder.transform(df.id1)\n",
    "    df['id2_'] = label_encoder.transform(df.id2)\n",
    "    gt_df['node_id'] = label_encoder.transform(gt_df.user_id)\n",
    "    \n",
    "    \n",
    "    all_node_features_id1 = get_dist(df_,'id1')\n",
    "    all_node_features_id2 = get_dist(df_,'id2')\n",
    "    early_node_features_df = pd.concat([get_dist(df,'id1_',rating_cols=all_node_features_id1.columns),\n",
    "                                                                       get_dist(df,'id2_',rating_cols=all_node_features_id2.columns)],1).fillna(0).sort_index()    \n",
    "    print(gt_df.label.value_counts())\n",
    "    # assert f'{data_name}_early' not in os.listdir('input'), \"Directory already exists\"\n",
    "    # assert gt_df['label'].value_counts().min()>=10, \"Cannot do 10-fold cross validation\"\n",
    "    os.mkdir(f'input/{data_name}_early{rate}')\n",
    "    df[['id1_','id2_','weight']].to_csv(f'input/{data_name}_early{rate}/{data_name}_early{rate}_network.csv',index=None)\n",
    "    gt_df[['node_id','label']].to_csv(f'input/{data_name}_early{rate}/{data_name}_early{rate}_gt.csv',index=None)\n",
    "    np.save(arr=label_encoder.classes_,file=f'input/{data_name}_early{rate}/{data_name}_early{rate}_label_encoder.npy')\n",
    "    early_node_features_df.to_csv(f'input/{data_name}_early{rate}/{data_name}_early{rate}_node_feature.csv',index=None)    \n",
    "    \n",
    "    \n",
    "    return df, gt_df, early_node_features_df, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early_network, early_gt, early_node_features, early_label_encoder = make_early_network('epinions',0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "file_path = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz'\n",
    "review_df_raw = pd.read_json(file_path,lines=True)\n",
    "assert ~review_df_raw.duplicated(['asin','reviewerID']).any()\n",
    "\n",
    "def preprocessing(df):\n",
    "    df_ = df.copy()\n",
    "    # converting 'reviewTime' column to datetime\n",
    "    df_['reviewTime'] = pd.to_datetime(df_.reviewTime,format='%m %d, %Y')\n",
    "    # vote_sum and helpful rate, helpful_bin\n",
    "    df_['vote_sum'] = df_['helpful'].map(lambda x:x[1])\n",
    "    df_['helpful_rate'] = df_['helpful'].map(lambda x:x[0]/x[1] if x[1]>0 else float('nan'))\n",
    "    df_['helpful_bin'] = pd.cut(df_.helpful_rate,bins=np.arange(0,1.1,0.1),include_lowest=True,labels=range(10))    \n",
    "    # おかしいデータを取り除く\n",
    "    df_ = df_.loc[~(df_.helpful_rate>1.0)]\n",
    "    return df_\n",
    "\n",
    "review_df = preprocessing(review_df_raw)\n",
    "\n",
    "def generate_network_csv(df, from_date):\n",
    "    review_df_from_ = df.loc[df.reviewTime>=from_date]\n",
    "    return review_df_from_[['reviewerID','asin','overall','reviewTime']]\n",
    "\n",
    "def generate_gt(df,from_date):\n",
    "    reviewer_all_votes = \\\n",
    "        df.loc[df.reviewTime>=from_date].groupby('reviewerID',as_index=False)['helpful'].agg(lambda x:list(np.vstack(x).sum(0)))\n",
    "\n",
    "    reviewer_all_votes['vote_sum'] = reviewer_all_votes.helpful.map(lambda x:x[1])\n",
    "\n",
    "    reviewer_all_votes['rate'] = reviewer_all_votes.helpful.map(lambda x:x[0]/x[1])\n",
    "    selected_df = reviewer_all_votes.loc[(reviewer_all_votes.vote_sum>=50) &\n",
    "                                         ((reviewer_all_votes.rate<=0.25) | \n",
    "                                          (reviewer_all_votes.rate>=0.75))]\n",
    "    selected_df['label'] = selected_df['rate'].map(lambda x: -1 if x <= 0.25 else 1)\n",
    "    \n",
    "    return selected_df[['reviewerID','label']].set_index('reviewerID')\n",
    "\n",
    "amazon_network = generate_network_csv(review_df,pd.Timestamp(2013,1,1))\n",
    "\n",
    "amazon_gt = generate_gt(review_df,pd.Timestamp(2013,1,1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appindix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## amazon user networkを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "self_joined = pd.merge(amazon_network,amazon_network,on='product_id',how='right')\n",
    "\n",
    "self_joined = self_joined.loc[~(self_joined.user_id_x==self_joined.user_id_y)]\n",
    "\n",
    "self_joined['sign'] = self_joined.weight_x*self_joined.weight_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_network = self_joined.loc[self_joined.sign!=0,['user_id_x','user_id_y','sign']]\n",
    "\n",
    "user_network = user_network.groupby(['user_id_x','user_id_y'],as_index=False)['sign'].mean().round()\n",
    "\n",
    "user_network = user_network.loc[user_network.sign!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(np.hstack((user_network.user_id_x,\n",
    "                                                   user_network.user_id_y,\n",
    "                                                   amazon_gt.user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_network['id1'] = label_encoder.transform(user_network.user_id_x)\n",
    "\n",
    "user_network['id2'] = label_encoder.transform(user_network.user_id_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_nx_network = user_network.copy()[['id1','id2','sign']]\n",
    "\n",
    "for_nx_network.columns = ['source','target','weight']\n",
    "\n",
    "G = nx.from_pandas_edgelist(for_nx_network,edge_attr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amazon_user_network = nx.to_pandas_edgelist(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_gt['node_id'] = label_encoder.transform(amazon_gt.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_user_network[['source','target','weight']].to_csv('input/amazon/user_network.csv',index=None)\n",
    "\n",
    "amazon_gt[['node_id','label']].to_csv('input/amazon/user_gt.csv',index=None)\n",
    "\n",
    "np.save(arr=label_encoder.classes_,file='input/amazon/user_label_encoder.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
