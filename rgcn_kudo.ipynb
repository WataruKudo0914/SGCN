{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from functools import partial\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
    "                 activation=None, is_input_layer=False):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "\n",
    "        # weight bases in equation (3)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
    "                                                self.out_feat))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels, self.num_bases))\n",
    "\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(self.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(self.w_comp,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(self.bias,\n",
    "                                    gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases (equation (3))\n",
    "            weight = self.weight.view(self.in_feat, self.num_bases, self.out_feat)\n",
    "            weight = torch.matmul(self.w_comp, weight).view(self.num_rels,\n",
    "                                                        self.in_feat, self.out_feat)\n",
    "        else:\n",
    "            weight = self.weight\n",
    "\n",
    "        if self.is_input_layer:\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                # embed = weight.view(-1, self.out_feat)\n",
    "                # index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                # index = edges.data['rel_type'] * self.in_feat + edges.src['id']\n",
    "                # return {'msg': embed[index] * edges.data['norm']}\n",
    "                w = weight[edges.data['rel_type']]\n",
    "                msg = torch.bmm(edges.src['init_h'].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data['norm']\n",
    "                return {'msg': msg}                \n",
    "        else:\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data['rel_type']]\n",
    "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data['norm']\n",
    "                return {'msg': msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data['h']\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {'h': h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg='msg', out='h'), apply_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define full R-GCN model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_nodes, h_dim, out_dim, num_rels,node_feature_array,\n",
    "                 num_bases=-1, num_hidden_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.node_feature_array = node_feature_array\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for idx in range(self.num_hidden_layers - 1):\n",
    "            h2h = self.build_hidden_layer(idx)\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        # features = torch.arange(self.num_nodes)\n",
    "        features = torch.from_numpy(self.node_feature_array)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(self.node_feature_array.shape[1], self.h_dim[0], self.num_rels, self.num_bases,\n",
    "                         activation=F.relu, is_input_layer=True)\n",
    "\n",
    "    def build_hidden_layer(self,idx):\n",
    "        return RGCNLayer(self.h_dim[idx], self.h_dim[idx+1], self.num_rels, self.num_bases,\n",
    "                         activation=F.relu)\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(self.h_dim[-1], self.out_dim, self.num_rels, self.num_bases,\n",
    "                         activation=partial(F.softmax, dim=1))\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            # g.ndata['id'] = self.features\n",
    "            g.ndata['init_h'] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle dataset\n",
    "~~~~~~~~~~~~~~~~\n",
    "In this tutorial, we use AIFB dataset from R-GCN paper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(df,col):\n",
    "    df_cnt = df.groupby([col]+['etype'])['time'].count().unstack(1,fill_value=0)\n",
    "    df_dist = pd.DataFrame(df_cnt.values / df_cnt.sum(1).values.reshape(-1,1),\n",
    "                                               columns=df_cnt.columns,\n",
    "                                               index=df_cnt.index)\n",
    "    return df_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# etype = {1,2,3,4,5}\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "amazon_network_ = pd.read_csv('/home2/kudo/SGCN/raw_data/amazon/amazon_network.csv',header=None)\n",
    "amazon_network_.columns = ['src_raw','dst_raw','etype','time']\n",
    "amazon_network_inv = amazon_network_[['dst_raw','src_raw','etype','time']]\n",
    "amazon_network_inv.columns = ['src_raw','dst_raw','etype','time']\n",
    "\n",
    "amazon_network = amazon_network_.append(amazon_network_inv)\n",
    "\n",
    "etype_encoder = LabelEncoder()\n",
    "amazon_network['etype'] = etype_encoder.fit_transform(amazon_network.etype)\n",
    "amazon_gt = pd.read_csv('/home2/kudo/SGCN/raw_data/amazon/amazon_gt.csv',header=None)\n",
    "amazon_gt.columns = ['node_id_raw','label']\n",
    "amazon_gt = amazon_gt.drop_duplicates('node_id_raw')\n",
    "\n",
    "# edge_normの計算\n",
    "amazon_src_cnt = amazon_network.groupby(['src_raw','etype'])['time'].count().unstack(1,fill_value=0)\n",
    "\n",
    "amazon_src_dist = pd.DataFrame(amazon_src_cnt.values/amazon_src_cnt.sum(1).values.reshape(-1,1),\n",
    "                                                        index=amazon_src_cnt.index,\n",
    "                                                        columns=amazon_src_cnt.columns)\n",
    "\n",
    "merged_network = pd.merge(amazon_network,amazon_src_dist.stack().reset_index(),on=['src_raw','etype'])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.hstack((merged_network.src_raw,\n",
    "                                                   merged_network.dst_raw,\n",
    "                                                   amazon_gt.node_id_raw)))\n",
    "\n",
    "merged_network['src'] = label_encoder.transform(merged_network.src_raw)\n",
    "\n",
    "merged_network['dst'] = label_encoder.transform(merged_network.dst_raw)\n",
    "\n",
    "amazon_gt['node_id'] = label_encoder.transform(amazon_gt.node_id_raw)\n",
    "amazon_gt['label'] = amazon_gt['label'].map(lambda x:1 if x==-1 else 0)\n",
    "\n",
    "# padding\n",
    "amazon_gt_padded = pd.merge(pd.DataFrame(np.arange(label_encoder.classes_.shape[0])),amazon_gt,\n",
    "                                      left_index=True,right_on='node_id',how='left').fillna(0.5).sort_values('node_id')\n",
    "\n",
    "num_nodes = label_encoder.classes_.shape[0]\n",
    "num_rels = merged_network.etype.unique().shape[0]\n",
    "num_classes = amazon_gt.label.unique().shape[0]\n",
    "labels = amazon_gt_padded['label'].values.astype(int).reshape(-1,1)\n",
    "all_idx = amazon_gt['node_id'].values\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(merged_network['etype'].values)\n",
    "edge_norm = torch.from_numpy(merged_network[0].values.astype('float32')).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)\n",
    "\n",
    "merged_network_directed = merged_network.copy().iloc[:merged_network.shape[0]//2,:]\n",
    "node_feature_df = pd.concat([get_dist(merged_network_directed,'src'),get_dist(merged_network_directed,'dst')],1).fillna(0).sort_index()\n",
    "node_feature_array = node_feature_df.values.astype('float32')\n",
    "\n",
    "known_labels = amazon_gt['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# etype = {-1,1}\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "amazon_network_ = pd.read_csv('/home2/kudo/SGCN/input/amazon/amazon_network.csv')\n",
    "\n",
    "amazon_network_.columns = ['src_raw','dst_raw','etype']\n",
    "amazon_network_inv = amazon_network_[['dst_raw','src_raw','etype']]\n",
    "amazon_network_inv.columns = ['src_raw','dst_raw','etype']\n",
    "\n",
    "amazon_network = amazon_network_.append(amazon_network_inv)\n",
    "amazon_network['time'] = 1\n",
    "\n",
    "etype_encoder = LabelEncoder()\n",
    "amazon_network['etype'] = etype_encoder.fit_transform(amazon_network.etype)\n",
    "\n",
    "amazon_gt = pd.read_csv('/home2/kudo/SGCN/input/amazon/amazon_gt.csv')\n",
    "amazon_gt.columns = ['node_id_raw','label']\n",
    "amazon_gt = amazon_gt.drop_duplicates('node_id_raw')\n",
    "\n",
    "# edge_normの計算\n",
    "amazon_src_raw_cnt = amazon_network.groupby(['src_raw','etype'])['dst_raw'].count().unstack(1,fill_value=0)\n",
    "\n",
    "amazon_src_raw_dist = pd.DataFrame(amazon_src_raw_cnt.values/amazon_src_raw_cnt.sum(1).values.reshape(-1,1),\n",
    "                                                        index=amazon_src_raw_cnt.index,\n",
    "                                                        columns=amazon_src_raw_cnt.columns)\n",
    "\n",
    "merged_network = pd.merge(amazon_network,amazon_src_raw_dist.stack().reset_index(),on=['src_raw','etype'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.hstack((merged_network.src_raw,\n",
    "                                                   merged_network.dst_raw,\n",
    "                                                   amazon_gt.node_id_raw)))\n",
    "\n",
    "merged_network['src'] = label_encoder.transform(merged_network.src_raw)\n",
    "\n",
    "merged_network['dst'] = label_encoder.transform(merged_network.dst_raw)\n",
    "\n",
    "amazon_gt['node_id'] = label_encoder.transform(amazon_gt.node_id_raw)\n",
    "amazon_gt['label'] = amazon_gt['label'].map(lambda x:1 if x==-1 else 0)\n",
    "\n",
    "# padding\n",
    "amazon_gt_padded = pd.merge(pd.DataFrame(np.arange(label_encoder.classes_.shape[0])),amazon_gt,\n",
    "                                      left_index=True,right_on='node_id',how='left').fillna(0.5).sort_values('node_id')\n",
    "\n",
    "num_nodes = label_encoder.classes_.shape[0]\n",
    "num_rels = merged_network.etype.unique().shape[0]\n",
    "num_classes = amazon_gt.label.unique().shape[0]\n",
    "labels = amazon_gt_padded['label'].values.astype(int).reshape(-1,1)\n",
    "all_idx = amazon_gt['node_id'].values\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(merged_network['etype'].values)\n",
    "edge_norm = torch.from_numpy(merged_network[0].values.astype('float32')).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)\n",
    "\n",
    "# merged_network_directed = merged_network.copy().iloc[:merged_network.shape[0]//2,:]\n",
    "# node_feature_df = pd.concat([get_dist(merged_network_directed,'src'),get_dist(merged_network_directed,'dst')],1).fillna(0).sort_index()\n",
    "# node_feature_array = node_feature_df.values.astype('float32')\n",
    "node_feature_df = pd.read_csv('/home2/kudo/SGCN/input/amazon/amazon_node_feature.csv')\n",
    "node_feature_array = node_feature_df.values.astype('float32')\n",
    "\n",
    "known_labels = amazon_gt['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha and otc Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "otc_network = pd.read_csv('/home2/kudo/SGCN/raw_data/otc/otc_network.csv',header=None)\n",
    "otc_network.columns = ['src_raw','dst_raw','etype','time']\n",
    "etype_encoder = LabelEncoder()\n",
    "otc_network['etype'] = etype_encoder.fit_transform(otc_network.etype)\n",
    "otc_gt = pd.read_csv('/home2/kudo/SGCN/raw_data/otc/otc_gt.csv',header=None)\n",
    "otc_gt.columns = ['node_id_raw','label']\n",
    "otc_gt = otc_gt.drop_duplicates('node_id_raw')\n",
    "\n",
    "# edge_normの計算\n",
    "otc_src_cnt = otc_network.groupby(['src_raw','etype'])['time'].count().unstack(1,fill_value=0)\n",
    "\n",
    "otc_src_dist = pd.DataFrame(otc_src_cnt.values/otc_src_cnt.sum(1).values.reshape(-1,1),\n",
    "                                                        index=otc_src_cnt.index,\n",
    "                                                        columns=otc_src_cnt.columns)\n",
    "\n",
    "merged_network = pd.merge(otc_network,otc_src_dist.stack().reset_index(),on=['src_raw','etype'])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.hstack((merged_network.src_raw,\n",
    "                                                   merged_network.dst_raw,\n",
    "                                                   otc_gt.node_id_raw)))\n",
    "\n",
    "merged_network['src'] = label_encoder.transform(merged_network.src_raw)\n",
    "\n",
    "merged_network['dst'] = label_encoder.transform(merged_network.dst_raw)\n",
    "\n",
    "otc_gt['node_id'] = label_encoder.transform(otc_gt.node_id_raw)\n",
    "otc_gt['label'] = otc_gt['label'].map(lambda x:1 if x==-1 else 0)\n",
    "\n",
    "# padding\n",
    "otc_gt_padded = pd.merge(pd.DataFrame(np.arange(label_encoder.classes_.shape[0])),otc_gt,\n",
    "                                      left_index=True,right_on='node_id',how='left').fillna(0.5).sort_values('node_id')\n",
    "\n",
    "num_nodes = label_encoder.classes_.shape[0]\n",
    "num_rels = merged_network.etype.unique().shape[0]\n",
    "num_classes = otc_gt.label.unique().shape[0]\n",
    "labels = otc_gt_padded['label'].values.astype(int).reshape(-1,1)\n",
    "all_idx = otc_gt['node_id'].values\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(merged_network['etype'].values)\n",
    "edge_norm = torch.from_numpy(merged_network[0].values.astype('float32')).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)\n",
    "\n",
    "node_feature_df = pd.concat([get_dist(merged_network,'src'),get_dist(merged_network,'dst')],1).fillna(0).sort_index()\n",
    "node_feature_array = node_feature_df.values.astype('float32')\n",
    "\n",
    "known_labels = otc_gt['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#etype = {-1,1}\n",
    "\n",
    "alpha_network = pd.read_csv('/home2/kudo/SGCN/input/alpha/alpha_network.csv')\n",
    "alpha_network.columns = ['src_raw','dst_raw','etype']\n",
    "etype_encoder = LabelEncoder()\n",
    "alpha_network['etype'] = etype_encoder.fit_transform(alpha_network.etype)\n",
    "alpha_gt = pd.read_csv('/home2/kudo/SGCN/input/alpha/alpha_gt.csv')\n",
    "alpha_gt.columns = ['node_id_raw','label']\n",
    "alpha_gt = alpha_gt.drop_duplicates('node_id_raw')\n",
    "alpha_network['time'] = 1\n",
    "\n",
    "# edge_normの計算\n",
    "alpha_src_cnt = alpha_network.groupby(['src_raw','etype'])['time'].count().unstack(1,fill_value=0)\n",
    "\n",
    "alpha_src_dist = pd.DataFrame(alpha_src_cnt.values/alpha_src_cnt.sum(1).values.reshape(-1,1),\n",
    "                                                        index=alpha_src_cnt.index,\n",
    "                                                        columns=alpha_src_cnt.columns)\n",
    "\n",
    "merged_network = pd.merge(alpha_network,alpha_src_dist.stack().reset_index(),on=['src_raw','etype'])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.hstack((merged_network.src_raw,\n",
    "                                                   merged_network.dst_raw,\n",
    "                                                   alpha_gt.node_id_raw)))\n",
    "\n",
    "merged_network['src'] = label_encoder.transform(merged_network.src_raw)\n",
    "\n",
    "merged_network['dst'] = label_encoder.transform(merged_network.dst_raw)\n",
    "\n",
    "alpha_gt['node_id'] = label_encoder.transform(alpha_gt.node_id_raw)\n",
    "alpha_gt['label'] = alpha_gt['label'].map(lambda x:1 if x==-1 else 0)\n",
    "\n",
    "# padding\n",
    "alpha_gt_padded = pd.merge(pd.DataFrame(np.arange(label_encoder.classes_.shape[0])),alpha_gt,\n",
    "                                      left_index=True,right_on='node_id',how='left').fillna(0.5).sort_values('node_id')\n",
    "\n",
    "num_nodes = label_encoder.classes_.shape[0]\n",
    "num_rels = merged_network.etype.unique().shape[0]\n",
    "num_classes = alpha_gt.label.unique().shape[0]\n",
    "labels = alpha_gt_padded['label'].values.astype(int).reshape(-1,1)\n",
    "all_idx = alpha_gt['node_id'].values\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(merged_network['etype'].values)\n",
    "edge_norm = torch.from_numpy(merged_network[0].values.astype('float32')).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)\n",
    "\n",
    "node_feature_df = pd.read_csv('/home2/kudo/SGCN/input/alpha/alpha_node_feature.csv')\n",
    "node_feature_array = node_feature_df.values.astype('float32')\n",
    "\n",
    "known_labels = alpha_gt['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create graph and model\n",
    "~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "n_hidden = [32,16] # number of hidden units\n",
    "n_bases = -1 # -1 # use number of relations as number of bases\n",
    "n_hidden_layers = 2 # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 150 # epochs to train\n",
    "lr = 0.001 # learning rate\n",
    "l2norm = 0.01 # L2 norm coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "\n",
    "for i, (for_train_val_idx, for_test_idx) in enumerate(kf.split(np.arange(len(all_idx)),y=known_labels)):\n",
    "    train_val_idx = all_idx[for_train_val_idx]\n",
    "    train_idx, val_idx = train_test_split(train_val_idx,test_size=0.33,stratify=known_labels[for_train_val_idx])\n",
    "    test_idx = all_idx[for_test_idx]\n",
    "    # create graph\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    g.add_edges(merged_network['src'].values, merged_network['dst'].values)\n",
    "    g.edata.update({'rel_type': edge_type, 'norm': edge_norm})\n",
    "    # create model\n",
    "    model = Model(len(g),\n",
    "                  n_hidden,\n",
    "                  num_classes,\n",
    "                  num_rels,\n",
    "                  node_feature_array,\n",
    "                  num_bases=n_bases,\n",
    "                  num_hidden_layers=n_hidden_layers)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "    print(\"Start {}-th fold\".format(i))\n",
    "    print(\"==== Train Phase ====\")\n",
    "    model.train()\n",
    "    best_auc = 0.0\n",
    "    best_auc_logits = None\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model.forward(g)\n",
    "        loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_auc = roc_auc_score(y_true=labels[train_idx].detach().numpy(),y_score=logits[train_idx].detach().numpy()[:,1])\n",
    "        train_loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "        val_auc = roc_auc_score(y_true=labels[val_idx].detach().numpy(),y_score=logits[val_idx].detach().numpy()[:,1])\n",
    "        val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "        \n",
    "        if val_auc >= best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_auc_logits = logits\n",
    "            \n",
    "        print(\"Epoch {:05d} | \".format(epoch) +\n",
    "              \"Train AUC: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "                  train_auc, loss.item()) +\n",
    "              \"Validation AUC: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "                  val_auc, val_loss.item()))\n",
    "    print(\"==== Test Phase ====\")\n",
    "    model.eval()\n",
    "    test_auc = roc_auc_score(y_true=labels[test_idx].detach().numpy(),y_score=best_auc_logits[test_idx].detach().numpy()[:,1])\n",
    "    auc_scores.append(test_auc)\n",
    "    print(\"test auc : {}\".format(test_auc))\n",
    "    print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果\n",
    "\n",
    "- (32,16) , カーネルは独立\n",
    "    - amazon : 0.749413165 (150 epochs)\n",
    "    - alpha : 0.926 (150 epochs)\n",
    "    - otc : 0.9601 (300 epochs)\n",
    "    \n",
    "- (32,16) , base = 3\n",
    "    - amazon : 0.7517083258204111\n",
    "    - alpha : 0.895\n",
    "    - otc : 0.95332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果\n",
    "\n",
    "- (32,16) , etype = {-1,1}\n",
    "    - amazon : 0.817841627 (150 epochs)\n",
    "    - alpha :  0.7928971028971029 (150 epochs)\n",
    "    - otc :  0.917612942612942 (150 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果\n",
    "\n",
    "- (32,16, 8) , カーネルは独立\n",
    "    - amazon : (150 epochs)\n",
    "    - alpha : 0.9206768231768232 (150 epochs)\n",
    "    - otc :  (300 epochs)\n",
    "    \n",
    "- (32,16,8) , base=3\n",
    "    - amazon : \n",
    "    - alpha : \n",
    "    - otc : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[val_idx], labels[val_idx])\n",
    "    val_acc = torch.sum(logits[val_idx].argmax(dim=1) == labels[val_idx])\n",
    "    val_acc = val_acc.item() / len(val_idx)\n",
    "    print(\"Epoch {:05d} | \".format(epoch) +\n",
    "          \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "              train_acc, loss.item()) +\n",
    "          \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "              val_acc, val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = logits[val_idx].detach().numpy()[:,1]\n",
    "\n",
    "y_true = labels[val_idx].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_true=labels[val_idx].detach().numpy(),y_score=logits[val_idx].detach().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(logits[val_idx].detach().numpy()[:,1][y_true==1],alpha=0.5)\n",
    "_ = plt.hist(logits[val_idx].detach().numpy()[:,1][y_true==0],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
